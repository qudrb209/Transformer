{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../Picture Data/logo.png\" alt=\"Header\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Copyright (C): 2010-2019, Shenzhen Yahboom Tech  \n",
    "@Author: Malloy.Yuan  \n",
    "@Date: 2019-07-17 10:10:02  \n",
    "@LastEditors: Malloy.Yuan  \n",
    "@LastEditTime: 2019-09-17 17:54:19  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object follow-Basic\n",
    "In this journey, we will show how to track objects using JetBot!\n",
    "We will use a pre-trained neural network trained on [COCO dataset] (http://cocodataset.org) to detect 90 different common objects. These include:\n",
    "\n",
    "* Person (index 0)\n",
    "* Cup (index 47)\n",
    "\n",
    "We need to import the ``ObjectDetector`` class, which uses our pre-trained SSD engine. There are many other classes (you can check [this file]\n",
    "(https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbt)\n",
    "Get a complete list of class indexes).\n",
    "\n",
    "This model comes from [TensorFlow Object Detection API]\n",
    "(https://github.com/tensorflow/models/tree/master/research/object_detection)\n",
    "\n",
    "The API also provides utilities for object detector training for custom tasks. Once the model is trained, we use the NVIDIA TensorRT to optimize the Jetson Nano. This makes the network very fast and can control Jetbot in real time.\n",
    "\n",
    "First, we need to import the \"ObjectDetector\" class, which uses our pre-trained SSD engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single camera image detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, the 'ObjectDetector' class uses the TensorRT Python API to execute the engine we provide. It is also responsible for pre-processing the input of the neural network and parsing the detected objects.\n",
    "\n",
    "Currently, it only works with engines created with the ``jetbotssd_tensorrt`` package. This package has utilities for converting models from the TensorFlow object detection API to the optimized TensorRT engine. Next, let's initialize the camera. Our detector needs 300x300 pixels of input, so we will set this parameter when creating the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "\n",
    "#camera = Camera.instance(width=224, height=224, fps=10)\n",
    "camera = Camera.instance(width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use some camera input to execute our network. By default, the ``ObjectDetector`` class expects the camera to generate a format of ``bgr8``.\n",
    "\n",
    "However, if the input format is different, you can override the default preprocessor function.\n",
    "If there are any COCO objects in the camera's field of view, they should now be stored in the ``detections`` variable, \n",
    "\n",
    "# Display detected objects in the text area\n",
    "\n",
    "we print them out by the code shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 3, 'confidence': 0.7065955400466919, 'bbox': [0.7674810886383057, 0.46338123083114624, 0.8945862054824829, 0.5225367546081543]}, {'label': 8, 'confidence': 0.6001883745193481, 'bbox': [0.3486727178096771, 0.20762529969215393, 0.7051639556884766, 0.5552468299865723]}, {'label': 3, 'confidence': 0.5464961528778076, 'bbox': [0.35549741983413696, 0.20767933130264282, 0.7034186720848083, 0.5503236651420593]}, {'label': 10, 'confidence': 0.34554681181907654, 'bbox': [0.7413750886917114, 0.2913311719894409, 0.7752922773361206, 0.3802035450935364]}, {'label': 3, 'confidence': 0.32209932804107666, 'bbox': [0.2339305877685547, 0.4818982183933258, 0.25664815306663513, 0.5073009729385376]}]]\n"
     ]
    }
   ],
   "source": [
    "detections = model(camera.value)\n",
    "\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6225b226cf3e46f19886c582c1783dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value=\"[[{'label': 3, 'confidence': 0.7065955400466919, 'bbox': [0.7674810886383057, 0.46338123083114…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "detections_widget = widgets.Textarea()\n",
    "detections_widget.value = str(detections)\n",
    "display(detections_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the label, confidence, and border position of each object detected in each image. In this example there is only one image (our camera).\n",
    "\n",
    "To print the first object detected in the first image, we can call the following command\n",
    "\n",
    ">If no object detected, an error may be appear here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'confidence': 0.8801805377006531, 'bbox': [0.0, 0.3776707649230957, 0.846367359161377, 0.8756875991821289]}\n"
     ]
    }
   ],
   "source": [
    "image_number = 0\n",
    "object_number = 0\n",
    "\n",
    "print(detections[image_number][object_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If no object is detected, an error may be appear here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control the robot to follow the center object\n",
    "\n",
    "Now, we want the robot to follow the object of the specified class. To do this, we will do the following work\n",
    "\n",
    "1. Detect objects that match the specified class\n",
    "2. Select the object closest to the center of the camera's field of view. This is the target object.\n",
    "3. Guide the robot to move to the target object, otherwise it will drift\n",
    "\n",
    "We will also create widgets that control the target object label, robot speed and cornering gain, and control the speed of the robot's cornering based on the distance between the target object and the center of the robot's field of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    #Image zoomed to 224,224 versus 224,244 obstacle avoidance model\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a robot instance of the drive motor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display all control widgets and connect the network execution function to the camera update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3ba63b96534d999264e04f5228bf52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'),)), IntText(value=1, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jetbot import bgr8_to_jpeg\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "speed_widget = widgets.FloatSlider(value=0.4, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget]),\n",
    "    label_widget,\n",
    "    speed_widget,\n",
    "    turn_gain_widget\n",
    "]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Calculate the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Calculate the length of a two-dimensional vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Find the detection closest to the center of the image\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "        \n",
    "def execute(change):\n",
    "    image = change['new']\n",
    "    # Calculate all detected objects\n",
    "    detections = model(image)\n",
    "    \n",
    "    # Draw all detected objects on the image\n",
    "    for det in detections[0]:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "    \n",
    "    # Select to match the detection of the selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    # Let the detection be closest to the center of the field and draw it\n",
    "    det = closest_detection(matching_detections)\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "    \n",
    "    # If don't detected object,jetbot car will keep advance\n",
    "    if det is None:\n",
    "        pass\n",
    "        robot.forward(0)\n",
    "        \n",
    "    # If detected object,control Jetbot follow object\n",
    "    else:\n",
    "        # Move the robot forward and control the x distance between the proportional target and the center\n",
    "        center = detection_center(det)\n",
    "        robot.set_motors(\n",
    "            float(speed_widget.value + turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value - turn_gain_widget.value * center[0])\n",
    "        )\n",
    "    \n",
    "    # Update image display to widget\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the detection function is called through the screen change before, the delay is relatively large, resulting in a poor following effect. We are temporarily using a loop to continuously call the detection function to improve the smoothness of the picture and improve the recognition effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera.unobserve_all()\n",
    "#camera.observe(execute, names='value')\n",
    "\n",
    "import threading\n",
    "import inspect\n",
    "import ctypes\n",
    "''' definition of the close thread function'''\n",
    "def _async_raise(tid, exctype):\n",
    "  \"\"\"raises the exception, performs cleanup if needed\"\"\"\n",
    "  tid = ctypes.c_long(tid)\n",
    "  if not inspect.isclass(exctype):\n",
    "    exctype = type(exctype)\n",
    "  res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, ctypes.py_object(exctype))\n",
    "  if res == 0:\n",
    "    raise ValueError(\"invalid thread id\")\n",
    "  elif res != 1:\n",
    "    # \"\"\"if it returns a number greater than one, you're in trouble,\n",
    "    # and you should call it again with exc=NULL to revert the effect\"\"\"\n",
    "    ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, None)\n",
    "    raise SystemError(\"PyThreadState_SetAsyncExc failed\")\n",
    "def stop_thread(thread):\n",
    "  _async_raise(thread.ident, SystemExit)\n",
    "'''The function of thread 1, which continuously calls the detection function'''\n",
    "def test():\n",
    "    while True:\n",
    "        execute({'new': camera.value})        \n",
    "\n",
    "thread1 = threading.Thread(target=test)\n",
    "thread1.setDaemon(False)\n",
    "thread1.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the robot is not blocked, you can see that the blue box surrounds the detected object and the target object (the object that the robot follows) will be displayed in green.\n",
    "\n",
    "When the target is discovered, the robot should turn to the target.\n",
    "\n",
    "You can call the following code block to manually disconnect the camera and stop the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "stop_thread(thread1)\n",
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
